
------------------------------------------------------------------------
kurzfristig
------------------------------------------------------------------------

Prozesse, Ad-Hoc-User-Accounts. Entweder mit nummerischen Unix-UIDs, die
einfach verwendet werden, oder mit einem Satz vorher eingerichteter
"echter" Unix-Accounts.
	Es gibt in Linux scheinbar drei UIDs:
	- EUID (effective): wird für Permission-Checks benutzt
	- RUID (real): ?
	- SUID (saved): dahin kann gewechselt werden; dadurch kann ein
		superuser-Prozess temporär unprivilegiert sein
http://www.gsp.com/cgi-bin/man.cgi?topic=setuid
http://en.wikipedia.org/wiki/User_identifier
http://man7.org/linux/man-pages/dir_section_2.html
	http://man7.org/linux/man-pages/man2/setuid.2.html
	sudo -u '#1234' bash
		-g group
		-n
			(non-interactive)
       -S
       		The -S (stdin) option causes sudo to read the password from the standard input instead of the terminal device.

--------------------------------

User-Supplied Programs. Sind die überhaupt wichtig? Stellt der Benutzer denn
*Programme* bereit oder *Plugins*? Vom Konzept her sind es Plugins für die
IDE, aber es ist unklar, ob diese als Programme besser realisierbar wären.
- Programme: Dann würde jeweils ein Programm für einen bestimmten Zweck
	gestartet und beendet, sobald dieser Zweck nicht mehr benötigt wird.
- Plugins: Dann würde jeweils ein CompProc als "Hilfsserver" gestartet, um
	einen oder mehrere Dienste anzubieten, und erst wieder beendet, wenn
	keiner der Dienste mehr benötigt wird. Analog zu Android-Activities.
	Die trotzdem nötige Main-Funktion würde vom System bereitgestellt,
	wobei sie Events auslösen würde.
	-
	Nachteil: IDE will Callback aufrufen, muss aber erst den CompProc
	starten und warten, bis dieser sich meldet, was er evtl. nicht tut.
	-> Es sollte nie an einem CompProc liegen, dass eine Funktion aufgehalten
	wird. So etwas muss man in der Plugin.json (ggf. mit deklarativen
	Preconditions) und per Scriptlets machen.
	-
	CompProc wird gestartet, um Aktionen durchzuführen, und das Asynchron.
	-> CompProc wird gestartet, um auf Events zu reagieren. Aber da tritt
	dann wieder das Problem auf, dass ein Event in der IDE darauf wartet,
	dass ein CompProc startet, damit der Event behandelt werden kann.
	-
	Wofür waren CompProc nochmal gedacht? Vor allem für SimVM. Da implementiert
	der CompProc einen Dokumentenserver und wenn er sich nicht an der IDE
	anmeldet, dann lädt der Dokumenten-"Editor" nicht. Das braucht die
	IDE nicht zu stören, solange der Rest damit klarkommt, dass der
	Editor (a) nicht lädt, oder (b) lädt aber danach eine leere Hülle
	bleibt. Beides könnte man analog zu dem Fall behandeln, dass das
	Laden des Dokumenteninhalts (z.B. des Texts bei einem Text-OT-Editor)
	länger dauert (z.B. HttpEditorSource).
	-
	In anderen Fällen als Dokumentenserver, wo ein CompProc evtl. sinnvoll
	wäre, aber kein CompProc vorgeschrieben, würde die IDE in erster
	Linie Scriptlets vorsehen. Wenn sich der CompProc nicht meldet dann
	müssten die Scriptlets damit klarkommen; falls nicht, verhalten sie
	sich schlimmstenfalls wie aufgehängte Scriptlets und auch mit denen
	muss die IDE sowieso klarkommen.
	-
	Das alles klärt aber immer noch nicht die Frage, ob CompProc jetzt
	als "Main" laufen oder als Server für "Extensions" (mit impliziter
	Main). Oder ob das überhaupt pauschal festgelegt werden sollte.
	Hier ist das Problem, dass man mehr über das Anwendungsgebiet
	wissen müsse, als "unter anderem für SimVM oder andere
	Dokumentenserver". Bei SimVM kommt das CompProc-Programm aus dem
	SimVM-Core und alles andere aus Modulen, die von Plugins bereitgestellt
	und über SimVM-Extensions eingebunden werden.
	-
	SimVM: Der CompProc läuft für den Workspace, nur 1x für alle User.
	Er muss aus dem ExtNet des WS kommen, was ja für einen Dokumentenserver
	auch Sinn macht. Aber er muss einen Service *pro Dokument* bereitstellen.
	Ein Prozess könnte also immer noch mehrere Dokumente serven. Das
	hätte aber Probleme mit der Abschirmung aus Stabilitätsgründen
	und zwei Sims in einem Prozess könnten sich auch Performancetechnisch
	stören. Ohne jetzt eine Aussage über CompProcs allgemein zu machen ist
	es für SimVM besser, einen eigenen Prozess je Dokument zu starten.
	-
	Damit ist für "CompProc für SimVM" alles gesagt. Auf "CompProc für
	Dokumentenserver" ausgeweitet erst mal dasselbe System; hier wäre
	neu zu klären, ob nicht mehrere Dokumente in einem Prozess Sinn
	machen (darüber könnte sogar Text-OT abgewickelt werden, insbesondere
	um Workspaces gegeneinander abzuschirmen, mit max. N Dokumenten je
	OT-Server und jeder Server beschränkt auf einen einzigen WS).
	-
	Auf "CompProc" allgemein ausgeweitet stellen sich weitere Fragen, die
	jetzt noch nicht geklärt werden sollten / können:
	- wie sieht es bei CompProc für einzelne User statt für WS aus?
	- 1 Companion Process pro *was*? Wann werden neue CompProc gestartet?
		Wie lange leben sie? 
	---
	Es gibt bisher außer SimVM keine Document-basierten Editoren.
	Vermutlich kann das gesamte Document-System auf "immer per DocServer"
	umgestellt werden (Begründung: sonst müssten Document-Typen ja
	direkt in der IDE implementiert sein).
	Editoren allgemein müssen nicht auf Document aufbauen:
	- Text/OT
	- Reine Viewer (z.B. Wave Viewer) -> Scriptlets
	Also bleibt das Editoren-System wie es ist, und nur das Document-System
	wird umgebaut. Das neue DocSystem erst mal parallel zum alten, bis es
	gut genug ist, dass SimVM rüber kann. Im neuen System gibt es
	nur den DocHub, Document usw. -- kein IDocumentBody, also keine
	spezifischen Klassen, die direkt benutzt werden. Über das
	UI-Protocol kommen dann solche rein, und der Document State wird
	komplett im DocServer gespeichert (deshalb braucht SimVM auch
	so einen großen Umbau).
	-
	Alternativ kann der Umbau vorerst mal so sein, dass der IDE-Seitige
	Teil der neuen Doc Systems ein IDocumentBody ist, seine UI der
	UI-Protocol-Server und der Body den DocServer startet und sich
	dahin verbindet. Letztendlich sollte aber alles umgebaut werden,
	weil das jetzige DocSys nicht Clusterfähig ist. Um Wicket voll
	auszunutzen, kann es ja trotzdem voll ausgebaute Wicket-Komponenten
	für Ecosim etc. geben, nur dass die dann vom DocServer aus über das
	UI Protocol erzeugt werden statt lokal.
	-
	Beim Clustering müssen sich außerdem die Webserver zum DocServer
	connecten, nicht andersrum.
	-
	class ServerDocumentBody implements IDocumentBody {}
		- erzeugt CompanionProcess
			-> wie/wann wird der beendet?
		- agiert als UI Protocol Server
	-
	Erstes Ziel: Einfacher Editor auf DocServer-Basis, kein OT, nur
	ein einfacher formularbasierter Editor mit GUI und load/save in
	JSON-Format. Wenn der läuft dann ist die Document-Grundstruktur
	soweit. Danach kann das UI Protocol ausgebaut werden.
	
SimulationThread entspricht dem Code im DocServer.
	SimulationModel -> das läuft alles im DocServer 
Ansonsten: Events und UI Protocol
SimulatedVirtualMachine -> ???
simvm.editor package -> ???

--------------------------------

(Companion) Process Plugins, also Plugins für andere Companion Processes.
Besserer Begriff, um Verwirrung zu vermeiden: Code Libraries, Code Modules,
Companion Process Modules (Sollten nicht das Extension-System duplizieren!)
Hier ist zu beachten, dass am Ende alle Plugins in einem Programm zusammen
finden und da dann ein "Dependency Hell" Problem auftreten kann. Das ist
aber nicht anders als wenn man z.B. alle möglichen Module in einer NodeJS
Installation zusammenwirft. Es ist im Gegenteil recht transparent, wenn
man pro Programm (*) die Plugins einbindet und Aliase vergibt, mit denen
die Module dann geladen werden können (NodeJS: require("alias")).
(*) Ggf. sinnvoll, in der plugin.json eine Syntax zu erlauben, mit der
eine "Installation" (eingebundene Programm-Module) für mehrere Programme
innerhalb eines IDE-Plugins benutzt werden können, um die plugin.json
kleiner zu halten: (installation (modules ...) (programs ...))
->
Egal, ob CompProc als "Main" oder als Server für Extensions laufen: Sie haben
den Charakter einer "Main", was das Zusammenfinden von Modulen angeht,
da sie die eigentliche Funktionalität des CompProc bereitstellen und
Module tun das nicht. Auch bei einem Server für Extensions wird erst
im CompProc-Programm festgelegt, welche Services bereitgestellt werden,
Module stellen nur den Code bereit bzw. "interne" Extensions für die
Main / den Server auf intra-Prozess-Code-Ebene.

--------------------------------

Es gibt auch immer noch die Möglichkeit, den CompProc "trusted" zu machen
und alles über Scripting in einer Sandbox abzuwickeln. CompProc wäre
trotzdem ein eigener Prozess wegen Clustering (er läuft auf einem
eigenen DocServer, nicht auf den Webservern). Frage, wie das UI funktioniert,
stellt sich trotzdem (UI Protocol, DocServer ist Webserver, ...?)

Hat aber Nachteile und sieht im Augenblick nicht so aus als wäre das nötig.

--------------------------------

Plugin-System der IDE neu designen
- plugin.json bleibt
- Java-Klassen in jetziger Form weg
- Ordner für IDE-JS-Scriptlets (Rhino): "scriptlets"
- kein globales Scriptlet, welches Funktionen registriert, sondern direkt
	die einzelnen Funktionen und aus plugin.json verlinkt(wenn möglich),
	da der Ladezeitpunkt des Plugins keine Rolle spielt und da deshalb
	kein Code dranhängen sollte (aber was ist z.B. mit Plugin-weitem
	Lazy Init?).
- Code für Companion Processes und PLugins für solche (JS, Java,
	was auch immer), z.B. "companion/<name>" aus plugin.json verlinkt

Wie geht Versionierung?
Abgesehen von Versionierung, Möglichkeiten für CompProc:
- 1 Prozess je Workspace
	Dann muss der Proc am ExtNet des WS hängen, sonst haklig und
	Probleme mit Versionierung
- 1 Prozess je ExtNetwork
	Dann laufen CompProc evtl. mehrfach je WS, und evtl. mit 
	verschiedenen Versionen. Trotzdem evtl. der sauberste Weg.
- ggf. 1 Prozess insgesamt (z.B. OT, aber das ist ein Spezialfall)

--------------------------------

UI Protocol. Was genau wird hier spezifiziert? Einfache UI-Komponenten,
klar, aber wo fließen Daten her/hin? Zwischen UI und Companion Processes?
Oder spezifiziert der CompProc auch einen Datenfluß zwischen UI und
IDE-Main? Wie immer gilt: Spezialfälle wie OT mal ausgenommen.

Das UI-Protocol sollte vorsehen, dass der DocServer das UI als Datenstruktur
aufbaut und sich dann *mehrere* Webserver connecten, um das darzustellen.
Nach Möglichkeit sollte die Struktur im DocServer multiclientfähig
sein, aber evtl. muss man da selbst eingreifen. In Java lässt sich
sowas gut struktureiren; NodeJS keine Ahnung. Erst mal lassen sich aber
die meisten GUI-Komponenten ohne per-Client-State bauen (auf der
DocServer-Seite heißt das; im Webserver gibt es sehr wohl per-Client-State;
die Struktur ist sehr ähnlich wie ein Stateless Webserver und AJAX,
nur dass ich hier das Protocol selbst definieren kann und den "Client",
also den Webserver, mit allen nötigen Features ausstatten kann).

Wie sehr lässt sich dieses Prinzip verallgemeinern? Im Zweifel wäre so
ein Wicket-basierter Webserver, hinter dem ein Stateless Content Server
steht, eine coole Sache auch für andere Projekte. Ein paar Überlegungen
dazu:
- für andere Projekte ist ein "Verfallsdatum" für die UI-Spec sinnvoll.
	Bei mir ist das nicht nötig, weil das UI-Protocol auf einer
	dauerhaften Verbindung und JSON-Packets aufbaut. Wenn sich
	serverseitig was ändert, dann wird das gepusht. Für andere Projekte
	müsste man das minimal abstrahieren: Die Verbindung versendet
	JSON-Packets, und in anderen Projekten gibt es kein Push, nur ein
	Pull von JSON-Daten und ggf. ein Verfallsdatum.
- klassische Webserver-Projekte würden sehr wenig auf "echte" Komponenten
	und mehr auf blankes HTML setzen, wobei sich mit (HTML mit darin
	vergrabenen Komponenten) schon viel erreichen lässt.
- der Webserver müsste eher ein erweiterbares Framework sein, weil in
	meinem Fall sehr viele Anwendungs-spezifische Komponenten und auch
	die direkte Anbindung an die eigentliche IDE nötig sind. Aber auch
	andere Projekte hätten sicher ihre spezifischen Anforderungen. Also
	eher ein Framework mit "Standardmodus".
- da sind sehr viele Bedingungen drin, also unklar ob das wirklich was
	wäre. Lieber erst mal nur meinen Fall bedenken, und der ist:
	DocumentServer und SimVM.

Alternativ könnte der "Content Server" stateful sein. Dann wäre das ganze
aus Sicht des CompProc wie NodeJS mit SocketIO, nur ggf. mit highlevel
UI components. Der Webserver würde daraus dann HTTP mit Atmosphere
machen und die einzelnen Stücke der verschiedenen CompProc oder
Scriptlets mergen. Dann kann man highlevel Components verwenden, wo drin
Widgets sind, die Callbacks verursachen, und Pushen geht auch. Oder
man embedded diese Widgets in HTML-Code, der vom Webserver so
ausgeliefert wird.

Nur passiert aus sicht des "Servers" (CompProc) vieles "clientseitig"
(im Wicket-Webserver), so dass nicht alles ständig zwischen den
Prozessen übertragen werden muss.

--------------------------------

Builders. Über Scriptlets und Companion Processes realisierbar, dazu
müssen die fertig spezifiziert sein, aber die Überlegungen zu Builders
sollten da einfließen.

--------------------------------
--------------------------------

Companion Processes sind Vergleichbar mit Application Packages in Android.
Einzelne Komponenten eines CompProc sind vergleichbar mit den Komponenten
einer Android-App, die plugin.json vergleichbar mit dem Android-Manifest.

Danach unterscheiden sich die beiden aber, z.B. machen abstrakte Intents
in der IDE nicht soviel Sinn, da passt das Eclipse-Plugin-System besser.
(Es gibt in der IDE selten die Situation, dass eine abstrakte Aktion
zur Verfügung steht und die konrekte Implementierung gesucht werden muss;
eher muss der Caller erst mal Anfragen welche Aktionen insgesamt
angeboten werden.) Entsprechend sind ContentProviders und Listeners
über das Eclipse-Plugin-System gut umsetzbar.

Desweiteren gehen obige Überlegungen immer dahin, mehrere CompProcs
je Plugin Bundle zu erlauben. Aber es gibt ja schon mehrere Plugin Bundles
je Plugin. (Trotzdem machen mehrere CompProc in manchen Fällen Sinn).
Aber solche Fälle sind selten; das könnte später erweitert werden, indem
man in der plugin.json auch Routing-Informationen einbaut. Bis dahin
macht vereinfachend Sinn: Jede *Verwendung* jedes Plugin Bundles (also
jedes Paar aus Plugin Bundle, Extension Network) hat genau einen CompProc.
(Und, zumindest by default, einen zugeordneten Ad-Hoc-Linux-User).
Per Einigung zwischen einem Plugin und seinen Bundles, oder zwischen
Verschiedenen Bundles können diese auch einen Linux-User gemeinsam
benutzen (Default wäre sinnvoller: Ein User je Plugin, nicht je Bundle).

Allerdings kollidiert das genau mit der Überlegung, einen neuen CompProc
für jede SimVM zu starten. Vielleicht sollte letzteres einfach manuell
aus dem jeweiligen Code passieren und nicht vom System kommen. Also so
dass vom System her erst mal das Dokument vom CompProc des SimVM-Plugins
behandelt wird, aber der SimVM Code selbst sagt: Ich starte einen neuen
Prozess für jedes Dokument und gebe der IDE einen Redirect dahin. Der
OT-Server würde das z.B. dann nicht tun, weil er mehrere Dokumente
managen kann. Der Redirect müsste dann direkt als Antwort auf ein
Document-Open kommen, alles andere wäre komplizierter als nötig.

Ähnlich wie Android-Apps werden auch die verschiedenen IDE-Plugins (bzw.
Bundles) per IPC *miteinander* kommunizieren müssen, nicht nur mit
der IDE. 

---

Wenn schon alles als "Betriebssystem" modelliert wird: Hinter den CompProcs
wird ein "Kernel" benötigt, um das alles zu managen. Tatsächlich sollte
man lieber von "Plugin Processes" statt "Companion Processes" sprechen,
und von IDE Processes als Oberbegriff für
- Prozesse mit Jetty/Wicket
- Plugin-Prozesse
- Queue Worker, die keine Plugin-Prozesse sind

und dieses "OS" läuft auf einem Cluster. Dafür muss ggf. auf jedem Node
ein Master-Prozess laufen. Die Master stimmen sich untereinander ab;
dabei ist ein Mega-Master eine Möglichkeit, eine gemeinsame SQL-
oder NoSQL-Datenbank wäre eine andere. Da es hier nur um die
Basis-Kommunikation der Master untereinander geht, sollte die Performance
kein Problem sein. In der DB könnte z.B. die Node- und Prozesstabelle
stehen. Auch die Plugins müssen zentral verteilt werden, wobei das
vermutlich aus Performancegründen getrennt sein sollte.

Einer der Anwendungsfälle wäre: Ein Webserver hat vom Benutzer den Befehl
bekommen, eine Datei zu öffnen. Der Editor für diese Datei ist kein reiner
Scriptlet-basierter Editor, sondern wird von einem Dokumenten-Prozess
aus einem Plugin gestützt. Dazu muss sich der Webserver mit dem
Plugin-Prozess verbinden, der für diese Datei Zuständig ist. Es kann
schon einen solchen geben, muss aber nicht. Dazu die Idee: Der Webserver
legt einen Task in eine Queue: "Ich brauche den Plugin-Prozess für das
Plugin Bundle X, Extension Network Y" (*). Ein freier Plugin-Prozess-Server
nimmt sich die Task, sieht zur Sicherheit nochmal nach, dass inzwischen
keiner durch einen parallelen Task gestartet wurde, startet den Prozess
und trägt ihn ein (Race Conditions beachten). Fall es doch schon einen
gab, wird dessen Info genommen. In beiden Fällen wird auf den Task returned,
wo der Prozess liegt (z.B. IP, Port). Dahin verbindet sich der Webserver
dann und hält die Verbindung offen, um das UI-Protocol zu fahren.

(*) Der Name "Extension Network" sollte nochmal geändert werden, da er
in diesem neuen Kontext auch eine viel bedeutendere Rolle hat.

Wie würde so etwas bei der Kommunikation zwischen zwei Plugin-Prozessen
aussehen? In welchen fällen würde die überhaupt stattfinden?





